{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# First Practical Laboratory: Deep Learning Architecture Experimentation\n",
                "\n",
                "**Machine Learning Technologies (MUCEIM)**\n",
                "\n",
                "**Student Name:** Borja Albert Gramaje\n",
                "\n",
                "**Date:** 23/11/2025\n",
                "\n",
                "---\n",
                "\n",
                "## Instructions\n",
                "\n",
                "1. **Make a copy** of this notebook to your own Google Drive (`File > Save a copy in Drive`) and be sure you select a Runtime with GPU\n",
                "2. **Fill in all empty code cells** as instructed\n",
                "3. **Document your analysis** in the markdown cells provided\n",
                "4. **Ensure the entire notebook runs** from top to bottom without errors (`Runtime > Restart and run all`)\n",
                "5. **Share the final notebook** with \"Anyone with the link can view\" and include the link in your PDF report\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section1"
            },
            "source": [
                "## 1. Import Libraries\n",
                "\n",
                "Import all necessary libraries for your experiments. Common libraries include TensorFlow/Keras, NumPy, Matplotlib, and Pandas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "imports"
            },
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, models, Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout\n",
                "from tensorflow.keras.callbacks import EarlyStopping\n",
                "from tensorflow.keras.optimizers import SGD, Adam\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "# Check TensorFlow version\n",
                "print(f\"TensorFlow version: {tf.__version__}\")\n",
                "\n",
                "# Check if GPU is available\n",
                "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "helper_functions"
            },
            "outputs": [],
            "source": [
                "# --- Helper Functions for Plotting ---\n",
                "def plot_acc(history, title=\"Model Accuracy\"):\n",
                "    plt.plot(history.history['accuracy'])\n",
                "    plt.plot(history.history['val_accuracy'])\n",
                "    plt.title(title)\n",
                "    plt.ylabel('Accuracy')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.legend(['Train', 'Val'], loc='upper left')\n",
                "    plt.show()\n",
                "\n",
                "def plot_loss(history, title=\"Model Loss\"):\n",
                "    plt.plot(history.history['loss'])\n",
                "    plt.plot(history.history['val_loss'])\n",
                "    plt.title(title)\n",
                "    plt.ylabel('Loss')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.legend(['Train', 'Val'], loc='upper right')\n",
                "    plt.show()\n",
                "\n",
                "def plot_compare_losses(history1, history2, name1=\"Model 1\",\n",
                "                        name2=\"Model 2\", title=\"Graph title\"):\n",
                "    plt.plot(history1.history['loss'], color=\"green\")\n",
                "    plt.plot(history1.history['val_loss'], 'r--', color=\"green\")\n",
                "    plt.plot(history2.history['loss'], color=\"blue\")\n",
                "    plt.plot(history2.history['val_loss'], 'r--', color=\"blue\")\n",
                "    plt.title(title)\n",
                "    plt.ylabel('Loss')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.legend(['Train ' + name1, 'Val ' + name1,\n",
                "                'Train ' + name2, 'Val ' + name2],\n",
                "               loc='upper right')\n",
                "    plt.show()\n",
                "\n",
                "def plot_compare_accs(history1, history2, name1=\"Model 1\",\n",
                "                      name2=\"Model 2\", title=\"Graph title\"):\n",
                "    plt.plot(history1.history['accuracy'], color=\"green\")\n",
                "    plt.plot(history1.history['val_accuracy'], 'r--', color=\"green\")\n",
                "    plt.plot(history2.history['accuracy'], color=\"blue\")\n",
                "    plt.plot(history2.history['val_accuracy'], 'r--', color=\"blue\")\n",
                "    plt.title(title)\n",
                "    plt.ylabel('Accuracy')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.legend(['Train ' + name1, 'Val ' + name1,\n",
                "                'Train ' + name2, 'Val ' + name2],\n",
                "               loc='lower right')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section2"
            },
            "source": [
                "## 2. Dataset Selection and Loading\n",
                "\n",
                "Choose your dataset from the options provided in the assignment document:\n",
                "- MNIST\n",
                "- Fashion MNIST\n",
                "- CIFAR-10\n",
                "- Custom dataset (with justification)\n",
                "\n",
                "Load and inspect the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load_data"
            },
            "outputs": [],
            "source": [
                "# Load Fashion MNIST dataset\n",
                "from tensorflow.keras.datasets import fashion_mnist\n",
                "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "inspect_data"
            },
            "outputs": [],
            "source": [
                "# Inspect the dataset\n",
                "print(f\"Training data shape: {x_train.shape}\")\n",
                "print(f\"Test data shape: {x_test.shape}\")\n",
                "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
                "\n",
                "# Visualize sample images\n",
                "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
                "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
                "\n",
                "plt.figure(figsize=(10,10))\n",
                "for i in range(25):\n",
                "    plt.subplot(5,5,i+1)\n",
                "    plt.xticks([])\n",
                "    plt.yticks([])\n",
                "    plt.grid(False)\n",
                "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
                "    plt.xlabel(class_names[y_train[i]])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dataset_justification"
            },
            "source": [
                "### Dataset Choice Justification\n",
                "\n",
                "**Dataset Selected:** Fashion MNIST\n",
                "\n",
                "**Justification:** I chose Fashion MNIST because it is a drop-in replacement for MNIST but offers a slightly more challenging classification task. While MNIST digits are very simple and can be classified with high accuracy by even simple linear models, Fashion MNIST images have more complex structures and textures, making it a better benchmark for observing the effects of architectural changes, regularization, and optimization strategies."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section3"
            },
            "source": [
                "## 3. Data Preprocessing\n",
                "\n",
                "Apply necessary preprocessing steps:\n",
                "- Normalization (e.g., scaling pixel values to [0,1])\n",
                "- One-hot encoding for labels (if needed)\n",
                "- Train/validation split\n",
                "- Any dataset-specific preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "preprocess"
            },
            "outputs": [],
            "source": [
                "# Preprocessing code\n",
                "\n",
                "# 1. Normalize pixel values to be between 0 and 1\n",
                "x_train = x_train / 255.0\n",
                "x_test = x_test / 255.0\n",
                "\n",
                "# 2. Reshape data (Flattening 28x28 images to 784 vectors for MLP)\n",
                "x_train = x_train.reshape(60000, 784)\n",
                "x_test = x_test.reshape(10000, 784)\n",
                "\n",
                "# 3. Cast to float32\n",
                "x_train = x_train.astype('float32')\n",
                "x_test = x_test.astype('float32')\n",
                "\n",
                "# 4. One-hot encoding of labels\n",
                "num_classes = len(np.unique(y_train))\n",
                "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
                "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
                "\n",
                "print(\"Preprocessing complete.\")\n",
                "print(f\"New x_train shape: {x_train.shape}\")\n",
                "print(f\"New y_train shape: {y_train.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section4"
            },
            "source": [
                "## 4. Baseline Model\n",
                "\n",
                "Define, compile, and train a simple baseline model. This will serve as your point of comparison for all subsequent experiments.\n",
                "\n",
                "**Baseline Architecture Description:**\n",
                "The baseline model is a Multi-Layer Perceptron (MLP) with 3 hidden layers using Sigmoid activation functions.\n",
                "- Input Layer: 784 neurons (flattened image)\n",
                "- Hidden Layer 1: 128 neurons, Sigmoid\n",
                "- Hidden Layer 2: 128 neurons, Sigmoid\n",
                "- Hidden Layer 3: 64 neurons, Sigmoid\n",
                "- Output Layer: 10 neurons, Softmax\n",
                "- Optimizer: SGD\n",
                "- Loss: Categorical Crossentropy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "baseline_model"
            },
            "outputs": [],
            "source": [
                "# Define baseline model\n",
                "def create_baseline_model():\n",
                "    model = Sequential()\n",
                "    model.add(Dense(128, activation='sigmoid', input_shape=(784,)))\n",
                "    model.add(Dense(128, activation='sigmoid'))\n",
                "    model.add(Dense(64, activation='sigmoid'))\n",
                "    model.add(Dense(num_classes, activation='softmax'))\n",
                "    return model\n",
                "\n",
                "model_baseline = create_baseline_model()\n",
                "model_baseline.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "baseline_compile"
            },
            "outputs": [],
            "source": [
                "# Compile baseline model\n",
                "model_baseline.compile(loss='categorical_crossentropy',\n",
                "                       optimizer='sgd',\n",
                "                       metrics=['accuracy'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "baseline_train"
            },
            "outputs": [],
            "source": [
                "# Train baseline model\n",
                "# Use EarlyStopping to prevent overfitting and save time\n",
                "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
                "\n",
                "print(\"Training Baseline Model...\")\n",
                "history_baseline = model_baseline.fit(x_train, y_train,\n",
                "                                      batch_size=32,\n",
                "                                      epochs=100,\n",
                "                                      validation_data=(x_test, y_test),\n",
                "                                      callbacks=[early_stopping],\n",
                "                                      verbose=1)\n",
                "\n",
                "# Plot baseline results\n",
                "plot_acc(history_baseline, title=\"Baseline Model Accuracy\")\n",
                "plot_loss(history_baseline, title=\"Baseline Model Loss\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section5"
            },
            "source": [
                "## 5. Systematic Experimentation (SGD Optimizer)\n",
                "\n",
                "Conduct experiments by modifying **one** aspect of the baseline at a time. In this section, we use **SGD** for all experiments."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "exp1_intro"
            },
            "source": [
                "### Experiment 1: Comparison of Activation Functions (Sigmoid vs ReLU)\n",
                "\n",
                "**Goal:** Compare the performance of Sigmoid (Baseline) vs ReLU activation functions.\n",
                "**Configuration:** Same architecture as baseline (128-128-64), but replace Sigmoid with **ReLU**. Optimizer: **SGD**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "exp1_code"
            },
            "outputs": [],
            "source": [
                "# Define Experiment 1 Model (ReLU)\n",
                "def create_exp1_model():\n",
                "    model = Sequential()\n",
                "    model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dense(64, activation='relu'))\n",
                "    model.add(Dense(num_classes, activation='softmax'))\n",
                "    return model\n",
                "\n",
                "model_exp1 = create_exp1_model()\n",
                "model_exp1.compile(loss='categorical_crossentropy',\n",
                "                   optimizer=SGD(learning_rate=0.01), # Explicitly set LR to avoid dying ReLU issues\n",
                "                   metrics=['accuracy'])\n",
                "\n",
                "print(\"Training Experiment 1 (ReLU)...\")\n",
                "history_exp1 = model_exp1.fit(x_train, y_train,\n",
                "                              batch_size=32,\n",
                "                              epochs=100,\n",
                "                              validation_data=(x_test, y_test),\n",
                "                              callbacks=[early_stopping],\n",
                "                              verbose=1)\n",
                "\n",
                "plot_acc(history_exp1, title=\"Exp 1: ReLU Accuracy\")\n",
                "plot_loss(history_exp1, title=\"Exp 1: ReLU Loss\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "exp2_intro"
            },
            "source": [
                "### Experiment 2: Effect of Network Depth\n",
                "\n",
                "**Goal:** Test if a deeper network improves performance with ReLU activation.\n",
                "**Configuration:** Increase hidden layers to 5 (128-128-128-128-64). Activation: **ReLU**. Optimizer: **SGD**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "exp2_code"
            },
            "outputs": [],
            "source": [
                "# Define Experiment 2 Model (Deep Network)\n",
                "def create_exp2_model():\n",
                "    model = Sequential()\n",
                "    model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dense(64, activation='relu'))\n",
                "    model.add(Dense(num_classes, activation='softmax'))\n",
                "    return model\n",
                "\n",
                "model_exp2 = create_exp2_model()\n",
                "model_exp2.compile(loss='categorical_crossentropy',\n",
                "                   optimizer='sgd',\n",
                "                   metrics=['accuracy'])\n",
                "\n",
                "print(\"Training Experiment 2 (Deep Network)...\")\n",
                "history_exp2 = model_exp2.fit(x_train, y_train,\n",
                "                              batch_size=32,\n",
                "                              epochs=100,\n",
                "                              validation_data=(x_test, y_test),\n",
                "                              callbacks=[early_stopping],\n",
                "                              verbose=1)\n",
                "\n",
                "plot_acc(history_exp2, title=\"Exp 2: Deep Network Accuracy\")\n",
                "plot_loss(history_exp2, title=\"Exp 2: Deep Network Loss\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "exp3_intro"
            },
            "source": [
                "### Experiment 3: Effect of Dropout Regularization\n",
                "\n",
                "**Goal:** Test if Dropout helps reduce overfitting.\n",
                "**Configuration:** Baseline architecture (ReLU) with Dropout layers (rate=0.2) after each hidden layer. Optimizer: **SGD**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "exp3_code"
            },
            "outputs": [],
            "source": [
                "# Define Experiment 3 Model (Dropout)\n",
                "def create_exp3_model():\n",
                "    model = Sequential()\n",
                "    model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
                "    model.add(Dropout(0.2))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dropout(0.2))\n",
                "    model.add(Dense(64, activation='relu'))\n",
                "    model.add(Dropout(0.2))\n",
                "    model.add(Dense(num_classes, activation='softmax'))\n",
                "    return model\n",
                "\n",
                "model_exp3 = create_exp3_model()\n",
                "model_exp3.compile(loss='categorical_crossentropy',\n",
                "                   optimizer='sgd',\n",
                "                   metrics=['accuracy'])\n",
                "\n",
                "print(\"Training Experiment 3 (Dropout)...\")\n",
                "history_exp3 = model_exp3.fit(x_train, y_train,\n",
                "                              batch_size=32,\n",
                "                              epochs=100,\n",
                "                              validation_data=(x_test, y_test),\n",
                "                              callbacks=[early_stopping],\n",
                "                              verbose=1)\n",
                "\n",
                "plot_acc(history_exp3, title=\"Exp 3: Dropout Accuracy\")\n",
                "plot_loss(history_exp3, title=\"Exp 3: Dropout Loss\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_adam_experiments"
            },
            "source": [
                "## 6. Optimization Experiments (Adam Optimizer)\n",
                "\n",
                "In this section, we will repeat the previous three experiments (Activation, Depth, Regularization) but using the **Adam** optimizer instead of SGD. This will allow us to directly compare the impact of the optimizer on different architectures.\n",
                "\n",
                "**Adam (Adaptive Moment Estimation)** is generally considered a more robust and faster-converging optimizer than standard SGD."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "exp4_intro"
            },
            "source": [
                "### Experiment 4: ReLU Activation with Adam\n",
                "\n",
                "**Goal:** Compare the performance of the ReLU model (from Exp 1) when trained with Adam vs SGD.\n",
                "**Configuration:** 3 Hidden Layers (128-128-64), ReLU Activation, **Adam Optimizer**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "exp4_code"
            },
            "outputs": [],
            "source": [
                "# Define Experiment 4 Model (ReLU + Adam)\n",
                "def create_exp4_model():\n",
                "    model = Sequential()\n",
                "    model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dense(64, activation='relu'))\n",
                "    model.add(Dense(num_classes, activation='softmax'))\n",
                "    return model\n",
                "\n",
                "model_exp4 = create_exp4_model()\n",
                "model_exp4.compile(loss='categorical_crossentropy',\n",
                "                   optimizer='adam',\n",
                "                   metrics=['accuracy'])\n",
                "\n",
                "print(\"Training Experiment 4 (ReLU + Adam)...\")\n",
                "history_exp4 = model_exp4.fit(x_train, y_train,\n",
                "                              batch_size=32,\n",
                "                              epochs=100,\n",
                "                              validation_data=(x_test, y_test),\n",
                "                              callbacks=[early_stopping],\n",
                "                              verbose=1)\n",
                "\n",
                "plot_acc(history_exp4, title=\"Exp 4: ReLU + Adam Accuracy\")\n",
                "plot_loss(history_exp4, title=\"Exp 4: ReLU + Adam Loss\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "exp5_intro"
            },
            "source": [
                "### Experiment 5: Deep Network (5 Layers) with Adam\n",
                "\n",
                "**Goal:** Compare the performance of the Deep ReLU model (from Exp 2) when trained with Adam vs SGD.\n",
                "**Configuration:** 5 Hidden Layers (128-128-128-128-64), ReLU Activation, **Adam Optimizer**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "exp5_code"
            },
            "outputs": [],
            "source": [
                "# Define Experiment 5 Model (Deep ReLU + Adam)\n",
                "def create_exp5_model():\n",
                "    model = Sequential()\n",
                "    model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dense(64, activation='relu'))\n",
                "    model.add(Dense(num_classes, activation='softmax'))\n",
                "    return model\n",
                "\n",
                "model_exp5 = create_exp5_model()\n",
                "model_exp5.compile(loss='categorical_crossentropy',\n",
                "                   optimizer='adam',\n",
                "                   metrics=['accuracy'])\n",
                "\n",
                "print(\"Training Experiment 5 (Deep ReLU + Adam)...\")\n",
                "history_exp5 = model_exp5.fit(x_train, y_train,\n",
                "                              batch_size=32,\n",
                "                              epochs=100,\n",
                "                              validation_data=(x_test, y_test),\n",
                "                              callbacks=[early_stopping],\n",
                "                              verbose=1)\n",
                "\n",
                "plot_acc(history_exp5, title=\"Exp 5: Deep ReLU + Adam Accuracy\")\n",
                "plot_loss(history_exp5, title=\"Exp 5: Deep ReLU + Adam Loss\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "exp6_intro"
            },
            "source": [
                "### Experiment 6: Dropout Regularization with Adam\n",
                "\n",
                "**Goal:** Compare the performance of the Dropout model (from Exp 3) when trained with Adam vs SGD.\n",
                "**Configuration:** 3 Hidden Layers (128-128-64) with Dropout (0.2), ReLU Activation, **Adam Optimizer**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "exp6_code"
            },
            "outputs": [],
            "source": [
                "# Define Experiment 6 Model (Dropout + Adam)\n",
                "def create_exp6_model():\n",
                "    model = Sequential()\n",
                "    model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
                "    model.add(Dropout(0.2))\n",
                "    model.add(Dense(128, activation='relu'))\n",
                "    model.add(Dropout(0.2))\n",
                "    model.add(Dense(64, activation='relu'))\n",
                "    model.add(Dropout(0.2))\n",
                "    model.add(Dense(num_classes, activation='softmax'))\n",
                "    return model\n",
                "\n",
                "model_exp6 = create_exp6_model()\n",
                "model_exp6.compile(loss='categorical_crossentropy',\n",
                "                   optimizer='adam',\n",
                "                   metrics=['accuracy'])\n",
                "\n",
                "print(\"Training Experiment 6 (Dropout + Adam)...\")\n",
                "history_exp6 = model_exp6.fit(x_train, y_train,\n",
                "                              batch_size=32,\n",
                "                              epochs=100,\n",
                "                              validation_data=(x_test, y_test),\n",
                "                              callbacks=[early_stopping],\n",
                "                              verbose=1)\n",
                "\n",
                "plot_acc(history_exp6, title=\"Exp 6: Dropout + Adam Accuracy\")\n",
                "plot_loss(history_exp6, title=\"Exp 6: Dropout + Adam Loss\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_comparisons"
            },
            "source": [
                "## 7. Comprehensive Comparison\n",
                "\n",
                "### 7.1 Optimizer Comparison (SGD vs Adam)\n",
                "\n",
                "We compare the same architectures with different optimizers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "compare_optimizers"
            },
            "outputs": [],
            "source": [
                "# Compare Exp 1 (ReLU SGD) vs Exp 4 (ReLU Adam)\n",
                "plot_compare_accs(history_exp1, history_exp4, name1=\"Exp 1 (ReLU SGD)\", name2=\"Exp 4 (ReLU Adam)\", title=\"ReLU: SGD vs Adam\")\n",
                "\n",
                "# Compare Exp 2 (Depth SGD) vs Exp 5 (Depth Adam)\n",
                "plot_compare_accs(history_exp2, history_exp5, name1=\"Exp 2 (Depth SGD)\", name2=\"Exp 5 (Depth Adam)\", title=\"Depth: SGD vs Adam\")\n",
                "\n",
                "# Compare Exp 3 (Dropout SGD) vs Exp 6 (Dropout Adam)\n",
                "plot_compare_accs(history_exp3, history_exp6, name1=\"Exp 3 (Dropout SGD)\", name2=\"Exp 6 (Dropout Adam)\", title=\"Dropout: SGD vs Adam\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_final_table"
            },
            "source": [
                "### 7.2 Master Comparison Table\n",
                "\n",
                "Below is a summary of all experiments conducted."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "final_table_code"
            },
            "outputs": [],
            "source": [
                "results = {\n",
                "    \"Baseline (Sigmoid/SGD)\": model_baseline.evaluate(x_test, y_test, verbose=0),\n",
                "    \"Exp 1 (ReLU/SGD)\": model_exp1.evaluate(x_test, y_test, verbose=0),\n",
                "    \"Exp 2 (Depth/SGD)\": model_exp2.evaluate(x_test, y_test, verbose=0),\n",
                "    \"Exp 3 (Dropout/SGD)\": model_exp3.evaluate(x_test, y_test, verbose=0),\n",
                "    \"Exp 4 (ReLU/Adam)\": model_exp4.evaluate(x_test, y_test, verbose=0),\n",
                "    \"Exp 5 (Depth/Adam)\": model_exp5.evaluate(x_test, y_test, verbose=0),\n",
                "    \"Exp 6 (Dropout/Adam)\": model_exp6.evaluate(x_test, y_test, verbose=0)\n",
                "}\n",
                "\n",
                "print(f\"{'Experiment':<30} | {'Test Loss':<10} | {'Test Accuracy':<15}\")\n",
                "print(\"-\"*60)\n",
                "for name, metrics in results.items():\n",
                "    print(f\"{name:<30} | {metrics[0]:.4f}     | {metrics[1]*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_conclusion"
            },
            "source": [
                "## 8. Conclusion\n",
                "\n",
                "**Write your conclusions here based on the Master Comparison Table.**\n",
                "\n",
                "Consider discussing:\n",
                "1.  **Optimizer Impact:** How much did Adam improve performance compared to SGD across the different architectures? Did it converge faster?\n",
                "2.  **Activation Functions:** Did ReLU still outperform Sigmoid when using Adam? (Comparing Exp 4 vs Exp 5/6)\n",
                "3.  **Regularization:** Did Dropout combined with Adam (Exp 6) yield the best generalization (lowest difference between train/val accuracy)?\n",
                "4.  **Best Overall Model:** Which combination of architecture, activation, and optimizer achieved the highest test accuracy?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ai_usage"
            },
            "source": [
                "## 9. AI Usage Documentation\n",
                "\n",
                "**Prompts used:**\n",
                "- \"Modify the notebook to use Fashion MNIST.\"\n",
                "- \"Create a baseline model with Sigmoid activation.\"\n",
                "- \"Create experiments for Depth, Dropout, and ReLU vs Sigmoid.\"\n",
                "- \"Repeat experiments using Adam optimizer and compare results.\"\n",
                "- \"Generate comparison plots and tables.\"\n",
                "\n",
                "**Modifications made:**\n",
                "- The code was structured into clear sections for each experiment.\n",
                "- Helper functions were used for consistent plotting.\n",
                "- A master comparison table was added to summarize all findings."
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}